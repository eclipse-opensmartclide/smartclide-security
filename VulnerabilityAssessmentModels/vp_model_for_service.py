# -*- coding: utf-8 -*-
"""
Created on Tue Jul 13 14:34:12 2021

@author: iliaskaloup
"""

#import numpy as np
import pandas as pd
import os, json, sys, re
from pathlib import Path
import tensorflow
from tensorflow.keras.models import load_model
#from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from keras_preprocessing.text import tokenizer_from_json
import tensorflow.keras.backend as K

def readFiles(inputPath, language):
    if language == "cpp":
        suffix = ".cpp"
    elif language == "java":
        suffix = ".java"
    elif language == "python":
        suffix = ".py"
    elif language == "javascript":
        suffix = ".js"
        
    paths = []
    for root, dirs, files in os.walk(inputPath):
        for file in files:
            if (file.endswith(suffix)):
                 #fullPath = os.path.join(root, file)
                 # print(fullPath)
                 # print(file)
                 if "Test".lower() not in str(file).lower():
                     root2 = Path(root)
                     paths.append(root2 / file)
    return paths

def createListOfTokens(codeFilename):
	"""
	Given a file name in the current working directory, read each line 
	and append it to a list.
	Return: the contents of the file as a list of strings
	"""
	codeLinesList = []

    #with open(codeFilename, "r", encoding='utf-8', errors='ignore') as fin:
    #with open(path, 'rb') as f:
	with open(codeFilename, "r", encoding='utf-8', errors='ignore') as fin:
		for line in fin:
			codeLinesList.append(line)

	return codeLinesList

def listToString(s): 
    
    # initialize an empty string
    str1 = "" 
    
    # traverse in the string  
    for ele in s: 
        str1 += ele  
    
    # return string  
    return str1 

def stringToList(string):
    codeLinesList = []
    for line in string.splitlines():
        codeLinesList.append(line)
    return codeLinesList

def remove_comments(content, language):
    if language == "cpp":
        def gen_content():
            block_comment = False
            line_comment = False
            probably_a_comment = False
            for character in content:
                if not line_comment and not block_comment and character == '/':
                    probably_a_comment = True
                    continue
    
                if block_comment and character == '*':
                    probably_a_comment = True
                    continue
    
                if line_comment and character == '\n':
                    line_comment = False
                    yield '\n'
                elif block_comment and probably_a_comment and character == '/':
                    block_comment = False
                elif not line_comment and not block_comment:
                    if probably_a_comment:
                        if character == '/':
                            line_comment = True
                        elif character == '*':
                            block_comment = True
                        else:
                            yield '/'
                            yield character
                    else:
                        yield character
                probably_a_comment = False
    
        return ''.join(gen_content())
    
    elif language == "java" or language == "javascript":
        commentsPattern = re.compile("(//.*?$)|(/\\*.*?\\*/)", flags=re.MULTILINE|re.DOTALL )
        stringsPattern = re.compile("(\".*?(?<!\\\\)\")")
             
        commentMatches = [ ]
        commentsToRemove = [ ]
         
        cmatcher = re.finditer( commentsPattern, content )
        smatcher = re.finditer( stringsPattern, content )
         
        for cm in cmatcher:
            commentMatches.append( cm )
            
        for sm in smatcher:
            for cm in commentMatches:
                if cm.start() > sm.start() and cm.start()<sm.end():
                    commentsToRemove.append( cm )
         
        for cm in commentsToRemove:
            commentMatches.remove( cm )
         
        for cm in commentMatches:
            content = content.replace( cm.group()," " )
        
        return content
    
    elif language == "python":
        content = re.sub('#.*', '', content)
        content = re.sub('""".*"""', '', content)          
        content = re.sub("'''.*'''", '', content)    
    
        doc_reg_1 = r'("""|\'\'\')([\s\S]*?)(\1\s*)(?=class)'
        doc_reg_2 = r'(\s+def\s+.*:\s*)\n(\s*"""|\s*\'\'\')([\s\S]*?)(\2[^\n\S]*)'
        content = re.sub(doc_reg_1, '', content)
        content = re.sub(doc_reg_2, r'\1', content)
    
        return content        

def dropHeaders(lines, language):
    linList = []
    for line in lines:
        if language == 'cpp':
            if not re.search('#include',line):
                 if not re.search('# include',line):
                       linList.append(line)
        elif language == 'java' or language == "javascript":
           x = re.search('^package', line)
           y = re.search('^import', line)
           if x == None:
               if y == None:
                   linList.append(line)
        elif language == "python":
           if not re.search('import',line):
             if not re.search('from',line):
                  linList.append(line)
    return linList

def dropBlank(tokens0):
    tokens = []
    for i in range(0, len(tokens0)):
        temp = tokens0[i]
        if temp != '':
            tokens.append(temp)
    return tokens

def tokenizeLines(codeLinesList):
	"""
	Tokenize each file line, i.e. entry of code list, based on a specified
    regex.
    ***The regex used is not applicable to every case. One needs to configure
    it based on file inputs.

	Return: list containing code tokens
	"""
    #enalaktika codeLinesList.split() gia na meinoun ta tokens mazi me to punctuation
	codeTokens = []

	for line in codeLinesList:
		templineTokens = re.split('[\.,\[\];:(\s)?\\\\!\t{}"<>+=~*&^%/|\\-\']', line)
		codeTokens.extend(templineTokens)

	return codeTokens

def getLengths(data):
    lens = []
    for i in range(len(data)):
        lens.append(len(data[i]))
    lens = pd.DataFrame(lens)
    return lens

def flatten(t):
    return [item for sublist in t for item in sublist]

def recall_metric(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())
        return recall

def precision_metric(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())
        return precision

def f1_metric(y_true, y_pred):

    prec = precision_metric(y_true, y_pred)
    rec = recall_metric(y_true, y_pred)
    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))
    return f1

def f2_metric(y_true, y_pred):

    prec = precision_metric(y_true, y_pred)
    rec = recall_metric(y_true, y_pred)
    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))
    return f2

def f2_loss(y_true, y_pred):

    y_true = tensorflow.cast(y_true, tensorflow.float32)
    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)
    #tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)
    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)
    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)

    p = tp / (tp + fp + K.epsilon())
    r = tp / (tp + fn + K.epsilon())

    f2 = 5*p*r / (4*p+r+K.epsilon())
    f2 = tensorflow.where(tensorflow.math.is_nan(f2), tensorflow.zeros_like(f2), f2)
    
    return 1 - K.mean(f2)

def loadTokenizer(language):
    if language == "cpp":
        tokenizer_path = "tokenizer_cpp.json"
    elif language == "java":
        tokenizer_path = "tokenizer_java.json"
    elif language == "javascript":
        tokenizer_path = "tokenizer_js.json"
    elif language == "python":
        tokenizer_path = "tokenizer_py.json"
    tokenizer_path = Path(tokenizer_path)
    with open(tokenizer_path) as f:
        dataTokenizer = json.load(f)
        tokenizer_obj = tokenizer_from_json(dataTokenizer)
    return tokenizer_obj

def modelLoader(language):
    if language == "cpp":
        model_path = "emb_model_cnn_cpp.h5"
        model_path = Path(model_path)
        myModel = load_model(model_path)
    elif language == "java":
        model_path = "emb_model_cnn_java.h5"
        model_path = Path(model_path)
        myModel = load_model(model_path)
    elif language == "javascript":
        model_path = "emb_model_cnn_js.h5"
        model_path = Path(model_path)
        myModel = load_model(model_path, custom_objects={"f2_metric": f2_metric})
    elif language == "python":
        model_path = "emb_model_cnn_py.h5"
        model_path = Path(model_path)
        myModel = load_model(model_path, custom_objects={"f2_loss": f2_loss, "f2_metric": f2_metric})
    return myModel


#main
def getVul(inputPath, language, project_name_time, date, commit_sha):
    
    kernel_size = 5
    #milli_sec1 = int(round(time.time() * 1000))
    
    #parse all files
    #print("\nTokenization is starting...\n")
    #read files c/cpp
    paths = readFiles(inputPath, language)
    #paths = paths[0:500]
    
    if len(paths) == 0:
        
        parsed = []
        jsonObj = ({"project_name": project_name_time, "date": date, "commit": commit_sha, "results": parsed})
        return jsonObj
        
        #return "EXIT"
        # sys.exit("There are no c, cpp, python, javascript or java files to analyze")
        
    #print("Load Tokenizer\n")
    tokenizer_obj = loadTokenizer(language)
        
    ## model
    #print("Load model\n")
    myModel = modelLoader(language)
    #print("Model summary: ", myModel.summary())
    #print("\n")
    #tf.keras.utils.plot_model(myModel, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

    classnames = []
    packages = []
    predScores = []
    predictions = []
    for i in range(0, len(paths)):
        path = str(paths[i])
        #path = Path(path)
        #print(path)
        if '\\' in path:
            path = path.replace('\\', '/')
        splitted = path.split(("/"))
        #print(splitted)
        classnames.append(splitted[-1])
        #print(path)
        #print(splitted)
        package = ""
        for i in range(1, len(splitted)-1):
            package+= "/" + splitted[i]
        packages.append(package)

        #tokenize source code in a list of lines
        lines0 = createListOfTokens(path)

        #convert source code from list of lines to string
        stringLines = listToString(lines0)

        stringLinesNoDigit = re.sub(r"$\d+\W+|\b\d+\b|\W+\d+$", "<numId$>", stringLines) #replace numbers

        stringLinesNoStr = re.sub(r'(["])(?:(?=(\\?))\2.)*?\1', "<strId$>", stringLinesNoDigit) #replace strings

        stringLinesNoChar = re.sub(r"(['])(?:(?=(\\?))\2.)*?\1", "<strId$>", stringLinesNoStr) #replace chars

        #remove comments from source code
        linesNoCom = remove_comments(stringLinesNoChar, language)

        #convert source code from string to list of lines
        lines = stringToList(linesNoCom)

        #remove headers
        lines = dropHeaders(lines, language)

        #tokenize lines to list of words
        tokens0 = tokenizeLines(lines)

        #remove blank lines
        tokens = dropBlank(tokens0)

        #print("Tokenization has been completed.\n")

        #print("Integer encoding and zero padding.\n")

        for w in range(0, len(tokens)):
            tokens[w] = tokens[w].lower()

        #seq_len = getLengths(lines)

        sequence = tokenizer_obj.texts_to_sequences(tokens)
        sequence = flatten(sequence)
        seq = []
        seq.append(sequence)
                
        if (len(seq[0]) > kernel_size):
            #print("Make predictions\n")
            #predictions = (myModel.predict(dataset) > 0.5).astype("int32")
            predScore = myModel.predict(seq)
            #print(seq)
            # patent to reduce probability for vulnerability existence without changing the '0.5' threshold
            '''if ((predScore[0] >= 0.5) and (predScore[0] <= 0.7)):
                newScore = predScore[0] - 0.2
            elif ((predScore[0] > 0.7) and (predScore[0] <= 0.75)):
                newScore = predScore[0] - 0.1
            else:
                newScore = predScore[0]'''
            newScore = predScore[0]
            if newScore >= 0.5:
                prediction = 1
            else :
                prediction = 0
            predScores.append(newScore[0])
            predictions.append(prediction)
        else:
            predScores.append(0.1)
            predictions.append(0)

    preds = pd.DataFrame(columns=['path','sigmoid', 'is_vulnerable', 'package', 'confidence', 'class_name'])
    
    paths2 = []
    for i in range(0, len(paths)):
        path = str(paths[i])
        path = path.replace('\\', '/')
        paths2.append(path)
    
    confs = []
    for i in range(0, len(predScores)):
        conf = predScores[i] * 100
        confInt = int(conf)
        confStr = str(confInt)
        confStr2 = confStr + " %"
        confs.append(confStr2)
    
    preds['path'] = paths2
    preds['sigmoid'] = predScores
    preds['is_vulnerable'] = predictions
    preds['package'] = packages
    preds['confidence'] = confs 
    preds['class_name'] = classnames
    
    #save preds to json
    result = preds.to_json(orient="records")
    parsed = json.loads(result)
    
    jsonObj = ({"project_name": project_name_time, "date": date, "commit": commit_sha, "results": parsed})
    
    '''jsonFilePath = "output.json"
    with open(jsonFilePath, 'w') as json_file:
        json.dump(jsonObj, json_file, indent=4)'''
    
    #milli_sec2 = int(round(time.time() * 1000))
    #print("\nEnd of analysis")
    #print("End of analysis after "+ str((milli_sec2-milli_sec1)/1000)+" seconds\n")
    return jsonObj


